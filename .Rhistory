if(length(extra_dates)>0){
avg_month = mean(site_df$predicted_hospitalized_country[format(site_df$date, "%m-%Y") == format(max(site_df$date), "%m-%Y")])
new_rows = site_df[rep(1, length(extra_dates)),]
new_rows$date = extra_dates
new_rows$predicted_hospitalized_country = avg_month
site_df = rbind(site_df, new_rows)
}
return(site_df)
}))
future$expected_hospitalized_site_observed_data_based = future$predicted_hospitalized_country/future$IHME_reduction_factor
future$expected_hospitalized_site_catchment_based = future$predicted_hospitalized_country/future$country_population * future$catchment_size
future$date_month = format(future$date, "%m-%Y")
res = aggregate(expected_hospitalized_site_observed_data_based~country + study_site + date_month,
FUN = sum, data=future, na.action=na.pass)
res$expected_hospitalized_site_observed_data_based = round(res$expected_hospitalized_site_observed_data_based)
res$expected_hospitalized_site_catchment_based = round(aggregate(expected_hospitalized_site_catchment_based~country + study_site + date_month,
FUN = sum, data=future, na.action = na.pass)[,4])
res = reshape(res, direction = 'wide', timevar = 'date_month',
v.names = c('expected_hospitalized_site_observed_data_based', 'expected_hospitalized_site_catchment_based'),
idvar = c('country', 'study_site'))
#This merge wouldn't have been needed if I could aggregate over missing values of covariates (which i can't coz you can't group NA)
res = merge(res, future[!duplicated(future$study_site), c('country', 'study_site', "catchment_size", "country_population",
"period.start", "period.end", "observed_hospitalized_site",
"IHME_predicted_in_past", "IHME_reduction_factor")],
by=c('country', 'study_site'))
colnames(res)
res = res[,c(1:2, 11:17,seq(3,9,2), seq(4,10,2))]
res$observed_period_length_months = c(as.numeric(res$period.end-res$period.start)/30)
res$observed_hospitalized_site_permonth = res$observed_hospitalized_site/res$observed_period_length_months
write.xlsx(x=res, file="output/predicted_hospitized_Apr_July_2021v3.xlsx", sheetName = 'predicted_hospitized_Apr_July_2021', row.names = F)
summary(1:100)
rexp(101, 1)
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
dist = rexp(n = 101, 1)
library(datasets)
rexp(n = 101, 10)
plot(rexp(n = 101, 10))
plot(rexp(n = 101, 0.5))
plot(rexp(n = 101, 0.25))
plot(rexp(n = 101, 0.1))
plot(rgamma(n = 101, shape = 0.1, scale = 0.1))
boxplot(rgamma(n = 101, shape = 0.1, scale = 10))
data(iq)
install.packages("usingR")
install.packages("UsingR")
library(UsingR)
data(iq)
force(iq)
boxplot(iq)
data("SAT")
SAT
boxplot(SAT$math)
boxplot(SAT$verbal)
ggplot() + geom_boxplot(aes(x="SAT Total", y=SAT$total))
ggplot(data=SAT) + geom_boxplot(aes(x="SAT Total", y=total))
SAT$total
data(SAT)
sat_total = SAT$total
sat_total = sort(sat_total)
sat_total = sort(c(sat_total), round(sat_total[1:25] + rnorm(n=25,mean = 0, sd = 5)))
round(sat_total[1:25] + rnorm(n=25,mean = 0, sd = 5))
SAT
rnorm(total_days, mean=36, sd =0.25)
rnorm(31 * 6, mean=36, sd =0.25)
boxplot(rnorm(31 * 6, mean=36, sd =0.25))
tail(1:10)
c(summary(1:10), sd(1:10))
(36 - 30 *0.8)/0.2
(36 - 30 *0.6)/0.2
(36 - 30 *0.9)/0.1
(36 - 30 *0.6)/0.4
(36 - 32 *0.6)/0.4
(8 - 6 *0.8)/0.2
(8 - 5 *0.8)/0.2
(8 - 5 *0.9)/0.1
(8 - 6 *0.9)/0.1
(8 - 6.5*0.9)/0.1
(8 - 6*0.85)/0.15
(8 - 5.5*0.85)/0.15
(8 - 5*0.8)/0.2
summary(rgamma(n=500, shape=20, rate=4))
summary(summary(rgamma(n=500, shape=20, rate=4)))
summary(rgamma(n=500, shape=20, rate=4))
summary(rgamma(n=500, shape=10, rate=2))
summary(rgamma(n=500, shape=50, rate=10))
summary(rgamma(n=500, shape=100, rate=20))
5 * 0.8 + 20 *0.2
working_time3 = rnorm(n=total_days-twenty_percent, 8, 3)
library(ggpubr)
set.seed(2021)
library(ggplot2)
library(UsingR)
library(ggpubr)
data(SAT)
sat_total = SAT$total
sat_total = sort(sat_total)
sat_total = sort(c(sat_total, round(sat_total[1:25] + rnorm(n=25,mean = 0, sd = 5))))
total_days = 31 * 6
twenty_percent = round(total_days*0.2)
working_time1 = pmin(24, c(rnorm(twenty_percent, 8-2.5, 0.25),
rnorm(total_days, mean=8, sd =0.25),
rnorm(twenty_percent, 8+2.5, 0.25)))
working_time2 = pmin(24, c(rnorm(n=total_days-twenty_percent, 5, 0.5),
rnorm(twenty_percent, mean=20, sd=0.5)))
working_time3 = pmin(24, rnorm(n=total_days-twenty_percent, 8, 4))
working_time1
working_time2
working_time3
summary(working_time3)
summary(working_time2)
summary(working_time1)
set.seed(2021)
library(ggplot2)
library(UsingR)
library(ggpubr)
data(SAT)
sat_total = SAT$total
sat_total = sort(sat_total)
sat_total = sort(c(sat_total, round(sat_total[1:25] + rnorm(n=25,mean = 0, sd = 5))))
total_days = 31 * 6
twenty_percent = round(total_days*0.2)
working_time1 = pmax(0,pmin(24, c(rnorm(twenty_percent, 8-2.5, 0.25),
rnorm(total_days, mean=8, sd =0.25),
rnorm(twenty_percent, 8+2.5, 0.25))))
working_time2 = pmax(0,pmin(24, c(rnorm(n=total_days-twenty_percent, 5, 0.5),
rnorm(twenty_percent, mean=20, sd=0.5))))
working_time3 = pmax(0, pmin(24, rnorm(n=total_days-twenty_percent, 8, 3.5)))
summary(working_time1)
summary(working_time2)
summary(working_time3)
8-2.5
(8 - 5*0.8)/0.2
(8 - 4*0.9)/0.1
(8 - 5*0.9)/0.1
(8 - 5.5*0.9)/0.1
(8 - 5.75*0.9)/0.1
(8 - 6*0.9)/0.1
(8 - 6.5*0.9)/0.1
(8 - 6*0.95)/0.05
(8 - 6.25*0.9)/0.1
runif(n=500, 0, 16)
summary(runif(n=500, 0, 16))
summary(runif(n=5000000, 0, 16))
0+16
mean(c(rep(0, 90), rep(1, 10)))
mean(c(rep(0.5, 90), rep(0.5, 10)))
mean(c(rep(0.25, 90), rep(0.25, 10)))
IQR(1:10)
quantile(1:10, probs = c(0.25, 0.75))
paste0("Median: ", median(1:10), ", IQR: ", quantile(1:10, probs = c(0.25, 0.75)))
paste0("Median: ", median(1:10), ", IQR: (", quantile(1:10, probs = c(0.25)), ", ", quantile(1:10, probs = c(0.25)))
paste0("Median: ", median(1:10), ", IQR: (", quantile(1:10, probs = c(0.25)), ", ", quantile(1:10, probs = c(0.25)), ")")
ggplot() + geom_bar()
library(ggplot2)
df <- data.frame(dose=c("D0.5", "D1", "D2"),
len=c(4.2, 10, 29.5))
head(df)
p<-ggplot(data=df, aes(x=dose, y=len)) +
geom_bar(stat="identity")
p
p$coordinates
p$coordinates()
p$coordinates$limits
p$coordinates$range
p$coordinates$range()
p$labels$x
p$coordinates
p$coordinates$render_axis_v
p$coordinates$distance
p$mapping
p<-ggplot(data=df, aes(x=dose, y=len)) +
geom_bar(stat="identity") + ylim(0, 50)
ggplot(data=df, aes(x=dose, y=len)) +
geom_bar(stat="identity") + ylim(0, 50)
p$layers
p$layers
p$layers
p$scales
p$labels
ggplot(data=df, aes(x=dose, y=len)) +
geom_bar(stat="identity", width = 0.5) + ylim(0, 50)
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = dose)
geom_bar(stat="identity", width = 0.5) + ylim(0, 50)
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = dose) +
geom_bar(stat="identity", width = 0.5) + ylim(0, 50)
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 0.5) + ylim(0, 50)
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 0.5) + ylim(0, 50) + xlab('ww')
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 1) + ylim(0, 50) + xlab('ww')
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 0.75) + ylim(0, 50) + xlab('ww')
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 1) + ylim(0, 50) + xlab('ww')
ggplot(data=df, aes(x=1:3, y=len)) + scale_x_continuous(breaks = 1:3, labels = df$dose) +
geom_bar(stat="identity", width = 0.5) + ylim(0, 50) + xlab('ww')
setwd("C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/")
setwd("C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/covid19dashboard/")
if(!require(readr)){
install.packages("readr")
library(readr)
}
if(!require(tidyverse)){
install.packages("tidyverse")
library(tidyverse)
}
if(!require(httr)){
install.packages("httr")
library(httr)
}
#########
# List of countries currently supported. Spellings are ultra important for all programs to work
# SA - south africa
# UK - united kingdom (you know this one I know)
# OWID - not a country but overall data of all countries from github
# IHME - data from IHME institute
#########
countries = c("Belgium", "Brazil", "Colombia", "Dominicana", "Germany", "Guatemala",
"Mexico", "Peru", "SA", "UK", "Poland", "Spain", "Philippines")
sapply(countries, FUN = function(country){
if(!dir.exists(file.path("2_DATA_raw/Country", country))){
dir.create(file.path("2_DATA_raw/Country", country), showWarnings = FALSE)
}
})
if(!dir.exists(file.path("2_DATA_raw/", "OWID"))){
dir.create(file.path("2_DATA_raw/", "OWID"), showWarnings = FALSE)
}
if(!dir.exists(file.path("2_DATA_raw/", "IHME"))){
dir.create(file.path("2_DATA_raw/", "IHME"), showWarnings = FALSE)
}
#################
# Some steps need to be done manually unfortunately:
# You need to download data of Brazil, South Africa, Poland, Philippines, and Guatemala manually
#### Guatemala ####
# We couldn't automate the step because the URL is dynamic
# We could emulate the download button link with RSelenium, but its time consuming work
# 1. Visit https://tablerocovid.mspas.gob.gt
#     1a. I know its an ugly website but please bear with me.
# 2. Click on Bases de datos
# 3. Download the link titled "Confirmados por municipio y fecha de inicio de sintomas".
# 4. Save the file as "DATA_PROCESSING\2_DATA_raw\Country\Guatemala\guatemala_cases.csv".
# 5. Download the link titled "Fallecidos por municipio y fecha de fallecimiento"
# 6. Save the file as "DATA_PROCESSING\2_DATA_raw\Country\Guatemala\guatemala_deaths.csv".
#### Brazil ####
# We couldn't automate the step because the URL is dynamic
# We could emulate the download button link with RSelenium, but its time consuming work
# 1. Visit https://covid.saude.gov.br
# 2. Click on the button download "Arquivo CSV"
#     2a. Wait, because it takes a minute or two for the download to start. It is a huge file.
#     2b. C'mon be patient. Ok! I'll be honest, I sometimes click twice in frustration, but a single click works.
# 3. Save the file as "DATA_PROCESSING\2_DATA_raw\Country\Brazil\brazil.csv"
#### South Africa ####
# We couldn't automate the step because the URL is dynamic
# We could emulate the download button link with RSelenium, but its time consuming work
# 1. Visit https://www.covid19sa.org/provincial-breakdown
# 2. Scroll down to the plot titled "Time Series Confirmed Cases"
# 3. Click On the top right corner where the three dots are, and a menu will appear.
# 4. Click on "Download CSV (Excel)"
#   4a. Make sure the downloading file is titled "[Website] Provincial Breakdown Page_Page 1_Combo chart.csv"
# 5. Save the file after renaming it as "DATA_PROCESSING\2_DATA_raw\Country\SA\SA_cases.csv"
# 6. Scroll down to the plot titled "Time Series Fatalities"
# 7. Do step 3 and 4 for this too.
# 8. Save the file after renaming it as "DATA_PROCESSING\2_DATA_raw\Country\SA\SA_deaths.csv"
###### Poland ######
#data is divided into two parts: upto 24th november 2020 in a CSV file
#and after 24th november via govt. website
#1. download city/powiat level data
#   https://docs.google.com/spreadsheets/d/1Tv6jKMUYdK6ws6SxxAsHVxZbglZfisC8x_HZ1jacmBM/edit#gid=1169869581
#2. download district level data
#   https://docs.google.com/spreadsheets/d/1ierEhD6gcq51HAm433knjnVwey4ZE5DCnu1bW7PRG3E/edit#gid=1841152698
#3. after 24th november 2020, data is available via another website
#   https://www.gov.pl/web/koronawirus/pliki-archiwalne-powiaty
###### Phillipines ######
#Link: http://bit.ly/DataDropArchives
#download Case_information file
####### Spain #########
#manually not needed
#https://cnecovid.isciii.es/covid19/#documentaci%C3%B3n-y-datos
##########
# At this point your manual download misery is over. I hope brazil got download?
# Anyway the rest of the code should work smoothly. If it doesn't, call 911.
#### Mexico url needs date of yesterday ####
mexico_refdate <- format(Sys.Date()-1, "%Y%m%d")
mexico_parent_url = "https://datos.covid-19.conacyt.mx/Downloads/Files/"
#the links matter, names don't. But I still give a name to know which link is for what
URLs = list("OWID"=c(url="https://codeload.github.com/owid/covid-19-data/zip/master",
ext=".zip",
dest='2_DATA_raw/OWID/covid-19-data-master.zip',
unzip_dir="2_DATA_raw/OWID",
junkpaths=F),
"IHME"=c(url="https://ihmecovid19storage.blob.core.windows.net/latest/ihme-covid19.zip",
ext='.zip',
dest="2_DATA_raw/IHME/ihme-covid19.zip",
unzip_dir="2_DATA_raw/IHME",
junkpaths=T),
"BE_CASES"=c(url="https://epistat.sciensano.be/Data/COVID19BE_CASES_AGESEX.csv",
ext='.csv',
dest="2_DATA_raw/Country/Belgium/COVID19BE_CASES_AGESEX.csv"),
"BE_DEATHS"=c(url="https://epistat.sciensano.be/Data/COVID19BE_MORT.csv",
ext=".csv",
dest="2_DATA_raw/Country/Belgium/COVID19BE_MORT.csv"),
"COLOMBIA"=c(url="https://www.datos.gov.co/api/views/gt2j-8ykr/rows.csv?accessType=DOWNLOAD",
ext='.csv',
dest="2_DATA_raw/Country/Colombia/Casos_positivos_de_COVID-19_en_Colombia.csv"),
"DOMINICAN_REPUBLIC"=c(url="https://github.com/gcaff/COVID19-RD/raw/master/data/covid_data_rd.csv",
ext='.csv',
dest="2_DATA_raw/Country/Dominicana/covid_data_rd.csv"),
"GERMANY"=c(url="https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv",
ext='.csv',
dest = "2_DATA_raw/Country/Germany/RKI_COVID19.csv"),
"MEXICO_NAT_CASES"=c(url=paste0(mexico_parent_url,"Casos_Diarios_Estado_Nacional_Confirmados_", mexico_refdate, ".csv"),
ext='.csv',
dest="2_DATA_raw/Country/Mexico/Casos_Diarios_Estado_Nacional_Confirmados.csv"),
"MEXICO_NAT_DEATHS"=c(url=paste0(mexico_parent_url, "Casos_Diarios_Estado_Nacional_Defunciones_", mexico_refdate, ".csv"),
ext='.csv',
dest="2_DATA_raw/Country/Mexico/Casos_Diarios_Estado_Nacional_Defunciones.csv"),
"MEXICO_MUNICIP_CASES"=c(url=paste0(mexico_parent_url, "Casos_Diarios_Municipio_Confirmados_", mexico_refdate, ".csv"),
ext='.csv',
dest="2_DATA_raw/Country/Mexico/Casos_Diarios_Municipio_Confirmados.csv"),
"MEXICO_MUNICIP_DEATHS"=c(url=paste0(mexico_parent_url, "Casos_Diarios_Municipio_Defunciones_", mexico_refdate, ".csv"),
ext='.csv',
dest="2_DATA_raw/Country/Mexico/Casos_Diarios_Municipio_Defunciones.csv"),
"PERU_CASES"=c(url="https://cloud.minsa.gob.pe/s/Y8w3wHsEdYQSZRp/download",
ext='.csv',
dest="2_DATA_raw/Country/Peru/positivos_covid.csv"),
"PERU_DEATHS"=c(url="https://cloud.minsa.gob.pe/s/Md37cjXmjT9qYSa/download",
ext='.csv',
dest = "2_DATA_raw/Country/Peru/fallecidos_covid.csv"),
"POLAND_CASES"=c(url="https://arcgis.com/sharing/rest/content/items/e16df1fa98c2452783ec10b0aea4b341/data",
ext='.zip',
dest="2_DATA_raw/Country/Poland/poland_after23nov.zip",
unzip_dir="2_DATA_raw/Country/Poland/after23_nov2020",
junkpaths=T),
"UK"=c(url="https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv",
ext='.csv',
dest = "2_DATA_raw/Country/UK/coronavirus-cases_latest.csv"),
"SPAIN_CASES_COMMUNES"=c(url="https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_ccaa.csv",
ext='.csv',
dest="2_DATA_raw/Country/Spain/covid_cases_communes.csv"),
"SPAIN_CASES_PROVINCE"=c(url="https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_provincia.csv",
ext='.csv',
dest="2_DATA_raw/Country/Spain/covid_cases_province.csv"))
for(i in 1:length(URLs)){
print(paste0("Downloading data for: ", names(URLs)[i]))
temp_file <- tempfile(fileext = URLs[[i]]['ext'])
req <- GET(URLs[[i]]['url'], write_disk(path = temp_file))
didRenameWork = file.rename(from = temp_file, to = URLs[[i]]['dest'])
if(didRenameWork == FALSE){
stop(paste0("File rename/move for ", names(URLs)[i] ,"data did not work. Is the source/destination file locked by another process (e.g., Rstudio)?"))
}
unlink(temp_file)
#unzip and delete the zip
if(URLs[[i]]['ext']=='.zip'){
unzip(zipfile = URLs[[i]]['dest'], exdir = URLs[[i]]['unzip_dir'], junkpaths = URLs[[i]]['junkpaths'])
unlink(URLs[[i]]['dest'])
}
}
#Dominican republic is included above, but this is just old code
#### Dominican Republic ####
# github_link <- "https://github.com/gcaff/COVID19-RD/raw/master/data/covid_data_rd.csv"
# temp_file <- tempfile(fileext = ".csv")
# req <- GET(github_link, authenticate(Sys.getenv("GITHUB_PAT"), ""), write_disk(path = temp_file))
# file.rename(from = temp_file, to = "2_DATA_raw/Country/Dominicana/covid_data_rd.csv")
# unlink(temp_file)
source("1_SCRIPTS/2_DATA_prep/1_OWID_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/2_IHME_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Belgium_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Brazil_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Colombia_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Dominicana_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Germany_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Guatemala_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Mexico_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Peru_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_SA_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_UK_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Spain_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Philippines_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/3_Poland_dataPrep.R")
source("1_SCRIPTS/2_DATA_prep/4_CountryCOMBINED_region.R")
saveRDS(object = dataCovid, file = "4_ShinyApp/DATA/dataCovidLocation.rds")
saveRDS(object = dat_age_cases_incidence, file="4_ShinyApp/DATA/dataCovidAgeIncidence.rds")
dat_age_cases_incidence = merge(x = dat_age_cases_incidence,
y = location_age_incidence,
by = "country", all = T)
####### Incidence rates by age_group
location_age_incidence = read.xlsx(file="1_SCRIPTS/2_DATA_prep/locations_age_incidence.xlsx", sheetIndex = 1)
dat_age_cases_incidence =do.call('rbind', lapply(location_age_incidence$country, FUN = function(country){
df = read.csv(file=paste0("3_DATA_processed/Country/", country,"/",country,"_region_COVID19_age.csv"))
df$time = as.numeric(unlist(sapply(table(df$age_group)-1, seq, from=0, by=1)))
total_months = length(unique(format(as.Date(df$date), "%B %Y")))
cumulative_cases = as.numeric(unlist(sapply(split(df$new_cases, df$age_group), FUN = function(newcases){
x=c(0, cumsum(newcases))
x = x[-length(x)]
return(x)
})))
df$population = df$population - cumulative_cases
#I think it overfitted, AIC BIC say otherwise, but model has 130 vcov parameters for 12 months
model_age_cases = glm(data=df, new_cases ~ age_group*ns(time, df = total_months), family = "poisson",
offset = log(population/100000))
lp_no_offset = model.matrix(~age_group*ns(time, df = total_months), data = df)
sdErr = sqrt(diag(lp_no_offset %*% vcov(model_age_cases) %*% t(lp_no_offset)))
df$mean_incidence_rate = exp(c(lp_no_offset %*% model_age_cases$coefficients))
df$low_incidence_rate = exp(qnorm(p=0.025, mean=log(df$mean_incidence_rate),sd = sdErr))
df$high_incidence_rate = exp(qnorm(p=0.975, mean=log(df$mean_incidence_rate),sd = sdErr))
return(df)
}))
dat_age_cases_incidence = merge(x = dat_age_cases_incidence,
y = location_age_incidence,
by = "country", all = T)
# saving output
saveRDS(object = dataCovid, file = "3_DATA_processed/Country/COMBINED/dataCovidLocation.rds")
saveRDS(object = dataCovid, file = "4_ShinyApp/DATA/dataCovidLocation.rds")
saveRDS(object = dat_age_cases_incidence, file="4_ShinyApp/DATA/dataCovidAgeIncidence.rds")
getwd()
#' @importFrom epiR epi.sscohortc
#' @importFrom utils combn
#' @export
get_cohort_mindet_VE = function(anticipated_brand_VEs=c(0.8, 0.5, 0.3),
overall_brand_proportions = c(0.3, 0.5, 0.2),
overall_vaccine_coverage=0.3,
attack_rate_unvaccinated = 0.1,
calculate_for_relative_VE = T,
power=0.8,
alpha=0.05,
confounder_adjustment_Rsquared = 0,
prob_missing_data = 0.1,
total_subjects=500){
if(!sum(overall_brand_proportions, na.rm = T)==1){
stop("Sum of brand proportions should be equal to 1")
}
total_vaccines = length(anticipated_brand_VEs)
relative_VE_combn = matrix(c(1:total_vaccines, rep(total_vaccines+1, total_vaccines)), byrow = T, nrow = 2)
if(calculate_for_relative_VE){
relative_VE_combn = cbind(combn(total_vaccines, 2), relative_VE_combn)
}
missing_data_adjusted_total_subjects = round(total_subjects * (1-prob_missing_data))
#the last index is for placebo (unvaccinated)
coverages = c(overall_brand_proportions * overall_vaccine_coverage, 1-overall_vaccine_coverage)
attack_rates = attack_rate_unvaccinated * (1 - c(anticipated_brand_VEs, 0))
mindet_VE = apply(relative_VE_combn, MARGIN = 2, function(vaccines){
group_coverage = sum(coverages[vaccines])
subpopulation_coverage = coverages[vaccines[1]] / group_coverage
#the 'n' parameter need not be integer for this API.
1 - epi.sscohortc(irexp1 = NA, irexp0 = attack_rates[vaccines[2]],
n = missing_data_adjusted_total_subjects * group_coverage * (1-confounder_adjustment_Rsquared),
power = power,
r = subpopulation_coverage/(1-subpopulation_coverage),
design = 1, sided.test = 2, conf.level = 1-alpha)$irr[1]
})
anticipated_brand_VEs = c(anticipated_brand_VEs, 0)
ret = data.frame(Vaccine1=paste("Vaccine", relative_VE_combn[1,]),
Vaccine2=ifelse(relative_VE_combn[2,]==total_vaccines+1, no = paste("Vaccine", relative_VE_combn[2,]), yes = "Unvaccinated"),
anticipated_VE = apply(relative_VE_combn, 2, FUN = function(x){
1 - (1-anticipated_brand_VEs[x[1]])/(1-anticipated_brand_VEs[x[2]])
}),
mindet_VE = mindet_VE)
return(ret)
}
debugSource('C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/VESS/R/get_cohort_mindet_VE.R')
source('C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/VESS/R/get_cohort_mindet_VE.R')
#' @importFrom epiR epi.sscohortc
#' @importFrom utils combn
#' @export
get_cohort_mindet_VE = function(anticipated_brand_VEs=c(0.8, 0.5, 0.3),
overall_brand_proportions = c(0.3, 0.5, 0.2),
overall_vaccine_coverage=0.3,
attack_rate_unvaccinated = 0.1,
calculate_for_relative_VE = T,
power=0.8,
alpha=0.05,
confounder_adjustment_Rsquared = 0,
prob_missing_data = 0.1,
total_subjects=500){
if(!sum(overall_brand_proportions, na.rm = T)==1){
stop("Sum of brand proportions should be equal to 1")
}else if(length(anticipated_brand_VEs)!=length(overall_brand_proportions)){
stop("Length of anticipated brand VE should be equal to length of overall brand proportions")
}
total_vaccines = length(anticipated_brand_VEs)
relative_VE_combn = matrix(c(1:total_vaccines, rep(total_vaccines+1, total_vaccines)), byrow = T, nrow = 2)
if(calculate_for_relative_VE){
relative_VE_combn = cbind(combn(total_vaccines, 2), relative_VE_combn)
}
missing_data_adjusted_total_subjects = round(total_subjects * (1-prob_missing_data))
#the last index is for placebo (unvaccinated)
coverages = c(overall_brand_proportions * overall_vaccine_coverage, 1-overall_vaccine_coverage)
attack_rates = attack_rate_unvaccinated * (1 - c(anticipated_brand_VEs, 0))
mindet_VE = apply(relative_VE_combn, MARGIN = 2, function(vaccines){
group_coverage = sum(coverages[vaccines])
subpopulation_coverage = coverages[vaccines[1]] / group_coverage
#the 'n' parameter need not be integer for this API.
1 - epi.sscohortc(irexp1 = NA, irexp0 = attack_rates[vaccines[2]],
n = missing_data_adjusted_total_subjects * group_coverage * (1-confounder_adjustment_Rsquared),
power = power,
r = subpopulation_coverage/(1-subpopulation_coverage),
design = 1, sided.test = 2, conf.level = 1-alpha)$irr[1]
})
anticipated_brand_VEs = c(anticipated_brand_VEs, 0)
ret = data.frame(Vaccine1=paste("Vaccine", relative_VE_combn[1,]),
Vaccine2=ifelse(relative_VE_combn[2,]==total_vaccines+1, no = paste("Vaccine", relative_VE_combn[2,]), yes = "Unvaccinated"),
anticipated_VE = apply(relative_VE_combn, 2, FUN = function(x){
1 - (1-anticipated_brand_VEs[x[1]])/(1-anticipated_brand_VEs[x[2]])
}),
mindet_VE = mindet_VE)
return(ret)
}
debugSource('C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/VESS/R/get_cohort_mindet_VE.R')
debugSource('C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/VESS/R/debug.R')
source('C:/Users/Anirudh Tomer/Dropbox (P95)/NotShared/VESS/R/debug.R')
